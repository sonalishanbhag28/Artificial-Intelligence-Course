{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Neural Network:\n",
    "Architecture where data gets processed layer by layer just like interconnected neurons (3 mil) in the human brain.\n",
    "Brain inputs- from sensory organs (sight, touch, smell, audio, etc)\n",
    "Only some neurons get excited on getting sensory input and understand what it is and where it is from \n",
    "    \n",
    "1. Biological neuron: dendrites(sending messages from sensory organs), cell body(nucleus), axon(transports message) and synapse(carries info to other neuron)\n",
    "2. Artificial Neuron or Perceptron: computing system inspired by biological neuron- three layers: \n",
    "        a. Input\n",
    "        b. Hidden- mathematical/other functionality\n",
    "        c. Ouput- categorical, continuous, binary etc\n",
    "\n",
    "Inputs -> Node -> Activation Function -> Outputs\n",
    "Input-Dendrites\n",
    "Node+Activation Function- Nucleus\n",
    "Weights- Synapse\n",
    "\n",
    "\n",
    "Steps for Neuron:\n",
    "1. Initialize the weights of input to near 0, but not 0. Then calculate the sum of product of these weights with input.\n",
    "2. This sum is passed through activation function.\n",
    "3. Activation output is resultant output.\n",
    "\n",
    "\n",
    "Weight: determines the importance of inputs. mathematically, just the slope of the line\n",
    "\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "They are used to transform non linear data to linearly separable data which helps to perform more complex tasks. Without AF, NN is just a linear regression network. Separates categories allowing easier prediction.\n",
    "\n",
    "1. Threshold Function\n",
    "2. Sigmoid Function\n",
    "3. Tanh Function- classification \n",
    "4. Softmax Function- categorical classification\n",
    "5. Relu Function\n",
    "\n",
    "## How does ANN learn?\n",
    "\n",
    "Regression kind of data:\n",
    "\n",
    "#### Rank HoursofStudy Percentage PredictedExam\n",
    "     2         12           84%       87%\n",
    "\n",
    "Inputs (2, 12, 84)\n",
    "\n",
    "    Step 1. Initialize weights as 0.2, 0.6, -0.2 => Compute SOP (weights,inputs)\n",
    "    Step 2. Activation Function: 2*0.2 + 12*0.6 + 84*-0.2=-9.2\n",
    "    Step 3. X-Relu(Rectified Linear Unit) (Sigma(-9.2)) => Output\n",
    "    Actual Value C=1/2(y^-Y)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to build ANN\n",
    "\n",
    "1. Import necessary libraries\n",
    "2. Initialize the model\n",
    "3. Add Input Layer (no of nodes, weight, activation, learning rate)\n",
    "4. Add Hidden Layers (no of nodes, weight, activation, learning rate)\n",
    "5. Add Output Layer (no of nodes, weight, activation, learning rate)\n",
    "6. Configure the learning processes:\n",
    "   (optimizer=sqd/batch, error= mse/bce/cce, accuracy=[mse or accuracy])\n",
    "7. Train the model- fit the model with xtrain and xtest\n",
    "8. Test the model with test set (x_test)\n",
    "9. Test the model with individual data (individual values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to Churn Modelling Data_Preprocessing.ipynb for coding part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colab Link: https://colab.research.google.com/drive/1bMjxJxiTDlLwx8cb2IWjjxrj5KuI1PMZ?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
